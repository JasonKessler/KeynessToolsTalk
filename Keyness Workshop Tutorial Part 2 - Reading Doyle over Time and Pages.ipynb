{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, '../scattertext/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import scattertext as st\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "\n",
    "assert st.version >= [0, 1, 17] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arthur Conan Doyle Corpus\n",
    "\n",
    "Theme corpus of the Using and Developing Software for Keyness Analysis workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cc3f1a541f496b9e2ef97805b96af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doyle_whole_df = pd.read_csv('doyle/metadata.csv', sep='\\t').assign(\n",
    "    Text = lambda df: df.idno.apply(lambda x: open(os.path.join('doyle/corpus', x + '.txt')).read()),\n",
    "    Parse = lambda df: df.Text.progress_apply(nlp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>subgenre</th>\n",
       "      <th>narration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StudyScarlet</td>\n",
       "      <td>1887</td>\n",
       "      <td>detective</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MysteryCloomber</td>\n",
       "      <td>1889</td>\n",
       "      <td>other</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SignFour</td>\n",
       "      <td>1890</td>\n",
       "      <td>detective</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FirmGirdlestone</td>\n",
       "      <td>1890</td>\n",
       "      <td>other</td>\n",
       "      <td>heterodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WhiteCompany</td>\n",
       "      <td>1891</td>\n",
       "      <td>historical</td>\n",
       "      <td>heterodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RafflesHaw</td>\n",
       "      <td>1891</td>\n",
       "      <td>horror</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Refugees</td>\n",
       "      <td>1893</td>\n",
       "      <td>historical</td>\n",
       "      <td>heterodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Parasite</td>\n",
       "      <td>1894</td>\n",
       "      <td>horror</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HoundBaskervilles</td>\n",
       "      <td>1902</td>\n",
       "      <td>detective</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LostWorld</td>\n",
       "      <td>1912</td>\n",
       "      <td>adventure</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PoisonBelt</td>\n",
       "      <td>1913</td>\n",
       "      <td>adventure</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ValleyFear</td>\n",
       "      <td>1915</td>\n",
       "      <td>detective</td>\n",
       "      <td>homodiegetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title  pubyear    subgenre       narration\n",
       "0        StudyScarlet     1887   detective    homodiegetic\n",
       "8     MysteryCloomber     1889       other    homodiegetic\n",
       "1            SignFour     1890   detective    homodiegetic\n",
       "9     FirmGirdlestone     1890       other  heterodiegetic\n",
       "4        WhiteCompany     1891  historical  heterodiegetic\n",
       "6          RafflesHaw     1891      horror    homodiegetic\n",
       "5            Refugees     1893  historical  heterodiegetic\n",
       "7            Parasite     1894      horror    homodiegetic\n",
       "3   HoundBaskervilles     1902   detective    homodiegetic\n",
       "10          LostWorld     1912   adventure    homodiegetic\n",
       "11         PoisonBelt     1913   adventure    homodiegetic\n",
       "2          ValleyFear     1915   detective    homodiegetic"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doyle_whole_df[['title', 'pubyear', 'subgenre', 'narration']].sort_values(by='pubyear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: explore summarizing Study Scarlet \n",
    "## Task 2: understand how the style of Doyle's writing changed\n",
    "\n",
    "Before beginning task 1, let's break up the books into chunks at most 500 words long. We'll ensure that chunks obey sentence boundaries so we can properly part-of-speech tag all text in each chunk.\n",
    "\n",
    "To do this, we use the `SentenceSequenceSegmenter` class.\n",
    "\n",
    "The chunking takes 1 minute 30 seconds on my 2018 MacbookPro.\n",
    "\n",
    "Note that we incldue linguistic process in our spaCy model (i.e., we are using \"en_core_web_sm\" instead of a \"blank\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8ad232a7634a8a9b1df53109feccf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b106ea52094421a38d70af4e194854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doyle_segment_df = st.SentenceSequenceSegmenter(\n",
    "    segment_length=500\n",
    ").whole_df_to_segmented_df(\n",
    "    df=doyle_whole_df,\n",
    "    parsed_col='Parse',\n",
    "    verbose=True\n",
    ").assign(\n",
    "    Parse = lambda df: df.Text.progress_apply(nlp)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge back in metadata about the other ACD books to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doyle_segment_augmented_df = pd.merge(\n",
    "    doyle_segment_df,\n",
    "    doyle_whole_df[['idno', 'author-name', 'author-gender', 'title',\n",
    "                    'pubyear', 'genre', 'subgenre', 'detective', 'horror', 'historical',\n",
    "                    'adventure', 'narration', 'availability', 'decade']], \n",
    "    left_on='DocIdx',\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "doyle_segment_augmented_df = pd.merge(\n",
    "    doyle_segment_augmented_df,\n",
    "    pd.DataFrame({'NumSegments':doyle_segment_augmented_df.groupby('title').SegmentIdx.max()}),\n",
    "    left_on='title',\n",
    "    right_index=True\n",
    ").assign(\n",
    "    Third = lambda df: (df.SegmentIdx/df.NumSegments).apply(\n",
    "        lambda x: 'First' if x < 1/3 else 'Second' if x < 2/3 else 'Third')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write crude HTML versions of each book, with chunk boundaries marked, so we can load them from the HTML files of our visiualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p book_html\n",
    "for title, book_df in doyle_segment_augmented_df.groupby('title'):\n",
    "    book_fn = f'book_html/{title}.html' \n",
    "    with open(book_fn, 'w') as of:\n",
    "        of.write(f'<h2>Title: {title}</h2>' +\n",
    "            '\\n'.join(book_df.apply(\n",
    "                lambda row: f'<h3><a name=\"section{row.SegmentIdx}\">Section {row.SegmentIdx}</a></h3><div style=\"white-space: pre-wrap;\">{str(row.Parse)}</div>',\n",
    "                axis=1\n",
    "            ))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offset-based feature identification\n",
    "\n",
    "As opposed to the unigram Part 1, we will experiment with n-gram features in when studying Doyle.\n",
    "\n",
    "We create an `OffsetCorpus`, which can include character offsets that indicate the location of features in the corpus. Like the lexicon memberships we saw in Part 1, these are considered \"non-text\" features.\n",
    "\n",
    "The `OffsetCorpusFactory` is used to build an offset corpus.\n",
    "\n",
    "A `FeatAndOffsetGetter` descendent, `FlexibleNGramFeatures`, is used to build these features. The class creates n-grams of the specified number of tokens from a document. It does not break sentence boundaries when extracting features.\n",
    "- The n-gram sizes are given in the `ngram_sizes` parameter. \n",
    "- Tokens are excluded from n-gram construction if they fail the `validate_token` filter. Since the start and end character offsets will be used to represent the found n-grams, filtered tokens can still be parts of these n-grams, though they will not be part of the name of the n-gram not count toward its size.\n",
    "- For each n-gram, a spaCy span is created, and the function `exclude_ngram_filter` is run to determine if the n-gram should be discarded. Here, we are excluding n-grams that do not start or end with a function word.\n",
    "\n",
    "After the corpus creation, there were 12,037 features created. As a rule of thumb, visualizing more than 4,000 features will result in a very long load time, and about 2,000 or under is ideal. Moreover, having multiple parts of a collocation present can serve as a distraction when examining the associations of these features.\n",
    "\n",
    "The next cells involve reducing this feature count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_number_matcher = re.compile('^[A-Za-z0-9]+$')\n",
    "\n",
    "def exclude_ngrams_which_do_not_start_and_end_with_function_words(ngram: spacy.tokens.Span) -> bool:\n",
    "    return any([\n",
    "        ngram[0].lower_.strip() in st.MY_ENGLISH_STOP_WORDS,\n",
    "        ngram[-1].lower_.strip() in st.MY_ENGLISH_STOP_WORDS,\n",
    "        word_number_matcher.match(ngram[0].lower_.strip()) is None,\n",
    "        word_number_matcher.match(ngram[-1].lower_.strip()) is None,\n",
    "        len(set([ngram[0].pos_, ngram[-1].pos_]) & {\"DET\", \"AUX\", \"ADP\", \"AUX\", \"PRON\", \"PUNCT\", 'CCONJ', 'PART'})\n",
    "    ])\n",
    "\n",
    "offset_feature_getter = st.FlexibleNGramFeatures(\n",
    "    ngram_sizes=[1, 2, 3, 4, 5],\n",
    "    exclude_ngram_filter = exclude_ngrams_which_do_not_start_and_end_with_function_words,\n",
    "    text_from_token = lambda tok: tok.lower_,\n",
    "    validate_token = lambda tok: tok.lower_.strip() != ''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('big', [1, [(15, 18)]]),\n",
       " ('test', [1, [(19, 23)]]),\n",
       " ('joe', [1, [(28, 31)]]),\n",
       " ('biden', [1, [(32, 37)]]),\n",
       " ('presidenet', [1, [(41, 51)]]),\n",
       " ('big test', [1, [(15, 23)]]),\n",
       " ('joe biden', [1, [(28, 37)]]),\n",
       " ('test and joe', [1, [(19, 31)]]),\n",
       " ('biden is presidenet', [1, [(32, 51)]]),\n",
       " ('big test and joe', [1, [(15, 31)]]),\n",
       " ('test and joe biden', [1, [(19, 37)]]),\n",
       " ('joe biden is presidenet', [1, [(28, 51)]]),\n",
       " ('big test and joe biden', [1, [(15, 37)]])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_feature_getter.get_metadata_offsets(nlp(\"This is a very big test and Joe Biden is presidenet.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sidebar: making custom feature representations\n",
    "\n",
    "This is implemented as a subclass of `FeatAndOffsetGetter`\n",
    "\n",
    "The class will contain a method `get_metadata_offsets`, which returns a list of tuples formatted as: \n",
    "\n",
    "```\n",
    "[\n",
    "    (Feature Name 1, [Count of number of occurrences, [\n",
    "        (Sample occurrence start character offset 1, Sample occurrence start character offset 2),\n",
    "        ...\n",
    "    ]),\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "This allows you to use feature names that may not appear in the text (lemmata, parts-of-speech, etc.) but are linked to examples. Note that this structure will be stored in memory when the visualization is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexFeatAndOffsetGetter(st.FeatAndOffsetGetter):\n",
    "    def __init__(self, regex_dict: Dict[str, str]):\n",
    "        '''\n",
    "        :param regex_dict: dict\n",
    "            This should be a dictionary mapping a label to the text of a regular expression\n",
    "            e.g., {'AorB': r'(a|b)', 'Number: r'\\d+'}\n",
    "        '''\n",
    "        self.labeled_regexes = [[k, re.compile(v, re.I | re.M)] for k, v in regex_dict.items()]\n",
    "\n",
    "    def get_metadata_offsets(\n",
    "        self, \n",
    "        doc: spacy.tokens.doc.Doc\n",
    "    ) -> List[Tuple[str, List[Union[int, List[Tuple[int, int]]]]]]:\n",
    "        text = str(doc)\n",
    "        offset_tokens = {}\n",
    "        for label, regex in self.labeled_regexes:\n",
    "            for match in regex.finditer(text):\n",
    "                token_stats = offset_tokens.setdefault(label, [0, []])\n",
    "                token_stats[0] += 1\n",
    "                token_stats[1].append(match.span())\n",
    "        return list(offset_tokens.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Detective', [2, [(0, 6), (12, 20)]])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexFeatAndOffsetGetter({'Detective':'(watson|sherlock)'}).get_metadata_offsets(\n",
    "    nlp(\"watson told sherlock that he was a being a little pedantic.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65309c9e2f3346fdb6e3fc47c4a294cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features in after initial creation 21486\n"
     ]
    }
   ],
   "source": [
    "studyscarlet_corpus = st.OffsetCorpusFactory(\n",
    "    doyle_segment_augmented_df[lambda df: df.title == 'StudyScarlet'],\n",
    "    category_col='SegmentIdx',\n",
    "    parsed_col='Parse',\n",
    "    feat_and_offset_getter=offset_feature_getter\n",
    ").build(\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print('# features in after initial creation', studyscarlet_corpus.get_num_terms(non_text=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply an NPMI (Gerlof 2009) scoring on all n-grams and, for each token length, keep the top 3000 n-grams. NPMI stands for normalized pointwise mutual information. \n",
    "\n",
    "Where PMI is the log ratio of a whole n-gram probability to the product of its component unigram probabilities, NPMI scales this ratio by the negative log probability of the entire n-gram. \n",
    "\n",
    "NPMI has the effect of ranking more frequent collocation candidates a little higher than PMI. \n",
    "\n",
    "Instead of setting an NPMI threshold, we select the K highest scoring n-grams of each length (other than 1), which occur at least twice. We will then go on to remove \n",
    "\n",
    "Bouma, Gerlof. Normalized (Pointwise) Mutual Information in Collocation Extraction. GSCL. 2009. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7b01f288bb48f7a54f4debd7ab2980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e11a48e55a64bed94137f1017d7e3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features after NPMI compaction 2600\n"
     ]
    }
   ],
   "source": [
    "npmi_studyscarlet_corpus = studyscarlet_corpus.compact(\n",
    "    compactor=st.NPMICompactor(\n",
    "        minimum_term_count = 2,\n",
    "        number_terms_per_length = 3000,        \n",
    "        show_progress=True,\n",
    "    ),\n",
    "    non_text=True\n",
    ")\n",
    "print('# features after NPMI compaction', npmi_studyscarlet_corpus.get_num_terms(non_text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then remove features which are either one character long or where 60% of their usages are in the same longer (token-wise) phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features after usage compaction and single-letter filtering 2455\n"
     ]
    }
   ],
   "source": [
    "filtered_npmi_studyscarlet_corpus = npmi_studyscarlet_corpus.compact(\n",
    "    st.NgramPercentageCompactor(\n",
    "        usage_portion=0.6, \n",
    "    ), \n",
    "    non_text=True\n",
    ").filter_out(\n",
    "    lambda x: len(x) == 1, \n",
    "    non_text=True\n",
    ")\n",
    "print(\"# features after usage compaction and single-letter filtering\", \n",
    "      filtered_npmi_studyscarlet_corpus.get_num_terms(non_text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the features retained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>NumTokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumTokens</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>155</th>\n",
       "      <td>said</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>man</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>holmes</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>190</th>\n",
       "      <td>sherlock holmes</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>jefferson hope</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>john ferrier</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>2161</th>\n",
       "      <td>salt lake city</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>said sherlock holmes</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>sitting - room</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>331</th>\n",
       "      <td>sprang to his feet</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>halliday 's private hotel</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>cried the little girl</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>1441</th>\n",
       "      <td>time , \" he said</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>way out of the station</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>holmes , \" he said</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     term  Frequency  NumTokens\n",
       "NumTokens                                                      \n",
       "1         155                        said        200          1\n",
       "          87                          man        148          1\n",
       "          188                      holmes         91          1\n",
       "2         190             sherlock holmes         45          2\n",
       "          1901             jefferson hope         31          2\n",
       "          2087               john ferrier         22          2\n",
       "3         2161             salt lake city          9          3\n",
       "          942        said sherlock holmes          7          3\n",
       "          530              sitting - room          5          3\n",
       "4         331          sprang to his feet          5          4\n",
       "          1794  halliday 's private hotel          5          4\n",
       "          2008      cried the little girl          3          4\n",
       "5         1441           time , \" he said          3          5\n",
       "          2437     way out of the station          2          5\n",
       "          956          holmes , \" he said          2          5"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Frequency': filtered_npmi_studyscarlet_corpus.get_freq_df(True).sum(axis=1)\n",
    "}).reset_index().assign(\n",
    "    NumTokens = lambda ser: ser.term.str.split().apply(len)\n",
    ").sort_values(by='NumTokens', ascending=False).groupby('NumTokens').apply(\n",
    "    lambda gdf: gdf.sort_values(by='Frequency', ascending=False).head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_metadata_factory(book_fn):\n",
    "    return lambda c: (f'<a href=\"{book_fn}#section' + c.get_df()['SegmentIdx'].astype(str) + '\"\" target=\"_\"> Segment ' \n",
    "                              + c.get_df()['SegmentIdx'].astype(str) \n",
    "                              + ' of ' + str(len(c.get_df()['SegmentIdx'])) + '</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterchron\n",
    "\n",
    "Let's create a Scatterchron plot of the sections in the compacted version of Study Scarlet. A lot of sections will be displayed in the parallel tag cloud at the top of the visualiztion. You can scroll horizontally on them.\n",
    "\n",
    "Below, terms are positioned at their average time stop, and their height is the dense rank of the DAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_categories = list(sorted(filtered_npmi_studyscarlet_corpus.get_categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.286139011383057\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1300\"\n",
       "            height=\"700\"\n",
       "            src=\"acd_studyscarlet_timeplot_table.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc0ee578d0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "html = st.produce_scattertext_table(\n",
    "    corpus=filtered_npmi_studyscarlet_corpus,\n",
    "    category_order=ordered_categories,\n",
    "    all_category_scorer=lambda corpus: st.AllCategoryTermScorer(corpus, term_scorer=st.DeltaJSDivergenceScorer),\n",
    "    metadata = book_metadata_factory('./book_html/StudyScarlet.html'),\n",
    "    top_terms_length=8,\n",
    "    ignore_categories=False,\n",
    "    plot_width=1000,\n",
    "    plot_height=400,\n",
    "    sort_doc_labels_by_name=True,\n",
    "    use_offsets=True,\n",
    "    non_text=True,\n",
    "    use_full_doc=True,\n",
    "    trend_plot_settings=st.TimePlotSettings(\n",
    "        category_order=ordered_categories,\n",
    "        dispersion_metric='DA',\n",
    "        dispersion_scaler=st.Scalers.dense_rank,\n",
    "        use_residual=False\n",
    "    )\n",
    ")\n",
    "print(time() - t0)\n",
    "fn = 'acd_studyscarlet_timeplot_table.html'\n",
    "with open(fn, 'w') as of:\n",
    "    of.write(html)\n",
    "\n",
    "IFrame(src=fn, width = 1300, height=700)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this issue, find the top 10 break points in the parallel tag cloud, and use them as cluster boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouper = st.CharacteristicGrouper(\n",
    "    filtered_npmi_studyscarlet_corpus, \n",
    "    non_text=True,\n",
    "    rank_embedder=st.RankEmbedder(\n",
    "        term_scorer=st.DeltaJSDivergenceScorer, \n",
    "        rank_threshold=10\n",
    "    ),\n",
    "    to_text=' to '\n",
    ")\n",
    "\n",
    "heading_categories, heading_category_order = grouper.get_new_doc_categories(\n",
    "    number_of_splits=16,\n",
    "    category_order=ordered_categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The resulting Scattechron is more easy to navigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.05072283744812\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1300\"\n",
       "            height=\"700\"\n",
       "            src=\"acd_studyscarlet_grouped_timeplot_table.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbac82281d0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "html = st.produce_scattertext_table(\n",
    "    corpus=filtered_npmi_studyscarlet_corpus,\n",
    "    category_order=ordered_categories,\n",
    "    heading_categories=heading_categories,\n",
    "    heading_category_order=heading_category_order,\n",
    "    all_category_scorer=lambda corpus: st.AllCategoryTermScorer(corpus, term_scorer=st.DeltaJSDivergenceScorer),\n",
    "    metadata = book_metadata_factory('./book_html/StudyScarlet.html'),\n",
    "    top_terms_length=8,    \n",
    "    ignore_categories=False,\n",
    "    plot_width=1000,\n",
    "    plot_height=400,\n",
    "    sort_doc_labels_by_name=True,\n",
    "    use_offsets=True,\n",
    "    non_text=True,\n",
    "    use_full_doc=True,\n",
    "    trend_plot_settings=st.TimePlotSettings(\n",
    "        category_order=ordered_categories,\n",
    "        dispersion_metric='DA',\n",
    "        dispersion_scaler=st.Scalers.dense_rank,\n",
    "        use_residual=False\n",
    "    )\n",
    ")\n",
    "print(time() - t0)\n",
    "fn = 'acd_studyscarlet_grouped_timeplot_table.html'\n",
    "with open(fn, 'w') as of:\n",
    "    of.write(html)\n",
    "\n",
    "IFrame(src=fn, width = 1300, height=700)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylistic change of Doyle over time\n",
    "We can model Doyle's style by looking at trigrams of either parts-of-speech or, in the case of closed-class words, their lowercased form.\n",
    "\n",
    "We filter out redundant features and pick the 2000 features with the higest JSD association.\n",
    "\n",
    "We display the Spearman correlation coefficient of each feature, as well as the highest log odds ratio terms during each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heading(c):\n",
    "    return (f'<a href=\"book_html/' + c.get_df()['title'] + '.html#section' + c.get_df()['SegmentIdx'].astype(str) + '\"\" target=\"_\"> Segment ' \n",
    "           + c.get_df()['SegmentIdx'].astype(str) \n",
    "           + ' of ' + str(len(c.get_df()['SegmentIdx'])) + '</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8139dd236f5d4969968ffe09cd85cba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features in after initial creation 89451\n"
     ]
    }
   ],
   "source": [
    "doyle_style_corpus = st.OffsetCorpusFactory(\n",
    "    doyle_segment_augmented_df,\n",
    "    category_col='pubyear',\n",
    "    parsed_col='Parse',\n",
    "    feat_and_offset_getter=st.FlexibleNGramFeatures(\n",
    "        ngram_sizes=[1, 2, 3],\n",
    "        text_from_token=(\n",
    "            lambda tok: (tok.tag_\n",
    "                         if (tok.lower_ not in st.MY_ENGLISH_STOP_WORDS\n",
    "                             or tok.tag_[:2] in ['VB', 'NN', 'JJ', 'RB', 'FW'])\n",
    "                         else tok.lower_)\n",
    "        )\n",
    "    )\n",
    ").build(\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print('# features in after initial creation', doyle_style_corpus.get_num_terms(non_text=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd1055004ad42c2a8a5b88e2428d93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb15035eb6c34d6a9c857a863cb6d634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doyle_style_corpus_compact = doyle_style_corpus.compact(\n",
    "    compactor=st.NPMICompactor(\n",
    "        minimum_term_count = 2,\n",
    "        number_terms_per_length = 3000,        \n",
    "        show_progress=True,\n",
    "    ),\n",
    "    non_text=True    \n",
    ").compact(\n",
    "    st.NgramPercentageCompactor(\n",
    "        usage_portion=0.6, \n",
    "    ), \n",
    "    non_text=True\n",
    ").compact(\n",
    "    compactor=st.AssociationCompactor(2000, scorer=st.DeltaJSDivergenceScorer, use_non_text_features=True),\n",
    "    non_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.264957904815674\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1300\"\n",
       "            height=\"700\"\n",
       "            src=\"acd_style_timeplot_table.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbb57835790>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "html = st.produce_scattertext_table(\n",
    "    corpus=doyle_style_corpus_compact,\n",
    "    category_order=ordered_categories,\n",
    "    all_category_scorer=lambda corpus: st.AllCategoryTermScorer(corpus, term_scorer=st.LogOddsRatio),\n",
    "    metadata = get_heading,\n",
    "    top_terms_length=8,    \n",
    "    ignore_categories=False,\n",
    "    plot_width=1000,\n",
    "    plot_height=400,\n",
    "    sort_doc_labels_by_name=True,\n",
    "    use_offsets=True,\n",
    "    non_text=True,\n",
    "    use_full_doc=True,\n",
    "    trend_plot_settings=st.CorrelationPlotSettings(\n",
    "        category_order=ordered_categories\n",
    "    )\n",
    ")\n",
    "print(time() - t0)\n",
    "fn = 'acd_style_timeplot_table.html'\n",
    "with open(fn, 'w') as of:\n",
    "    of.write(html)\n",
    "\n",
    "IFrame(src=fn, width = 1300, height=700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
